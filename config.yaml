common:
  seed: 12345
  batch_size: 2
  learning_rate: 3e-5
  num_train_epochs: 20
  dataset_path: ./data/
  data_limit: 5000 ## MUST CHANGE FOR FULL TRAIN
  max_token_length: 1024

wandb:
  project_name: learning2sumHF
  run_name: BART-cosine

model:
  save_path: ./saved_models
  save_name: cosine # model path would be like saved_models/MODEL_NAME/save_name

dataset:
  save_path: ./data
  sft_path: sft_data.json
  rm_path: rm_data.json
  rl_path: rl_data.json

sft:
  model_name: gpt2
  temperature: 0.5
  max_seq_length: 1024
  min_gen_length: 30
  max_gen_length: 120