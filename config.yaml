common:
  seed: 12345
  batch_size: 8
  learning_rate: 3e-5
  num_train_epochs: 15
  dataset_path: ./data/
  data_limit: None ## MUST CHANGE FOR FULL TRAIN
  max_token_length: 1024
  use_valid: True
  checkpoint_epoch: 5

wandb:
  use: True
  project_name: rlhf_dialogue
  run_name: RM

model:
  save_path: ./saved_models # model path would be like saved_models/MODEL_NAME/save_name
  checkpoint_name: None # ex) 5_checkpoint.pt or None

dataset:
  save_path: ./data
  sft_path: sft_data.json
  rm_path: rm_data.json
  rl_path: rl_data.json

sft:
  #model_name: EleutherAI/gpt-neo-1.3B
  #model_name: gpt2-large # 1.5B
  model_name: gpt2-medium # 345M
  temperature: 0.5
  max_seq_length: 1024
  min_gen_length: 30
  max_gen_length: 120
  learning_rate: 3e-5
  gradient_accumulation: 4 # If don't want to use, set it 1.

rm:
  model_name: roberta-large # 355M
  hidden_dim: 1024
  learning_rate: 3e-5
  gradient_accumulation: 1 # If don't want to use, set it 1.