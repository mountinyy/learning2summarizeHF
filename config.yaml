common:
  seed: 12345
  batch_size: 4
  num_train_epochs: 20
  dataset_path: ./data/
  data_limit: 1000 ## MUST CHANGE FOR FULL TRAIN
  max_token_length: 1024
  use_valid: True
  checkpoint_epoch: 9999

wandb:
  use: True  ## MUST CHECK
  project_name: rlhf_dialogue
  #run_name: RM(w.1_lm)_lr2e-6_data1000
  run_name: RL_data1000_kl0.1


model:
  save_path: ./models # model path would be like saved_models/MODEL_NAME/save_name
  checkpoint_name: None # ex) 5_checkpoint.pt or None

dataset:
  save_path: ./data
  sft_path: sft_data.json
  rm_path: rm_data.json
  rl_path: rl_data.json

sft:
  #model_name: EleutherAI/gpt-neo-1.3B
  #model_name: gpt2-large # 1.5B
  model_name: gpt2-medium # 345M
  temperature: 0.5
  max_seq_length: 1024
  min_gen_length: 30
  max_gen_length: 512
  learning_rate: 3e-5
  gradient_accumulation: 4 # If don't want to use, set it 1.

rm:
  #model_name: roberta-large # 355M
  model_name: facebook/bart-large # 400M
  #model_name: gpt2-medium # 345M
  saved_path : /home/sankm/rlhf_dialogue/models/sft_for_rm.pt # If you want to use saved SFT model, this should not be None
  use_saved_model: False
  hidden_dim: 1024
  learning_rate: 2e-6
  gradient_accumulation: 2 # If don't want to use, set it 1.

rl:
  actor_model_name: gpt2-medium # 345M
  actor_saved_path: /home/sankm/rlhf_dialogue/saved_models/sft/gpt2-medium/SFT-gpt2-medium/model_finished/model.pt
  actor_learning_rate: 2e-6
  critic_model_name: facebook/bart-large # 400M
  critic_saved_path: /home/sankm/rlhf_dialogue/saved_models/rm/facebook/bart-large/RM_lr2e-6_data1000/model_finished/model.pt
  critic_learning_rate: 2e-6
  episodes: 24 # 전체 episode
  max_timestep: 20 # 한 에피소드당 최대 10번의 experience를 만듬
  update_timestep: 10 # experience를 10번 만들 때마다 learn함
  kl_coef: 0.1
  max_epoch: 20 # Max Epoch만큼 memory를 샘플링하여 learn함.
  ppo_clip_ratio: 0.2
  value_clip_ratio: 0.4